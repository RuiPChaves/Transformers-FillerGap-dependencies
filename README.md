# Transformers-FillerGap-dependencies

Items, code, and analysis for 

Da Costa, Jillian K. and Rui P. Chaves (2020) "Assessing the ability of Transformer-based Neural Models to represent structurally unbounded dependencies", 3rd Annual Meeting of the Society for Computation in Linguistics (SCiL).
[https://www.acsu.buffalo.edu/~rchaves/transformer_fgs.pdf]

Code is not optimized.

Note that BERT and GPT2 contain extra experiments, not reported in the paper, that further show that GPT2 is superior to BERT.
- Experiment1 in NewExperiments.zip embeds the items in clauses with relatively short subject NPs. GTP2's accuracy remains excellent.
- Experiment2 NewExperiments.zip embeds the items in clauses with relatively large subject NPs. GTP2's accuracy remains excellent, unlike BERT's.
